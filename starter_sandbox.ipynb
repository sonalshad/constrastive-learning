{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:3'\n",
    "# torch.set_default_device(device) ## causes errors with dataset and loader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose(\n",
    "    [\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                        std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train = torchvision.datasets.CIFAR10(root='/home/sshad/data/wcl-data/', \n",
    "                                     train=True,\n",
    "                                     transform=transforms)\n",
    "trainloader = DataLoader(train, batch_size=batch_size, \n",
    "                         shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 3, 32, 32]), torch.Size([128]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.to(device)\n",
    "y = y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From WCL\n",
    "class GaussianBlur(object):\n",
    "    \"\"\"Gaussian blur augmentation in SimCLR https://arxiv.org/abs/2002.05709\"\"\"\n",
    "\n",
    "    def __init__(self, sigma=[.1, 2.]):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def __call__(self, x):\n",
    "        sigma = random.uniform(self.sigma[0], self.sigma[1])\n",
    "        x = x.filter(ImageFilter.GaussianBlur(radius=sigma))\n",
    "        return x\n",
    "\n",
    "\n",
    "# # Not sure how to use this with torch dataset. \n",
    "# class ContrastiveCIFAR10(Dataset): \n",
    "#     def __init__(self, train = True): \n",
    "#         super(Dataset, self).__init__\n",
    "\n",
    "#         self.data = CIFAR10(root='/home/sshad/data/wcl-data/', \n",
    "#                             train= train,\n",
    "#                             transform=None)\n",
    "#         type(self.data)\n",
    "#         ## SimCLR transformations\n",
    "#         self.transform = transforms.Compose([\n",
    "#             transforms.RandomResizedCrop(224),\n",
    "#             transforms.RandomHorizontalFlip(),\n",
    "#             transforms.RandomApply([transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)], p=0.8),\n",
    "#             transforms.RandomGrayscale(p=0.2),\n",
    "#             transforms.RandomApply([GaussianBlur([.1, 2.])], p=0.5),\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "#         ])\n",
    "\n",
    "#     def __len__(self): \n",
    "#         return len(self.data)\n",
    "    \n",
    "#     def __getitem__(self, index): \n",
    "#         image, label = self.data[index]\n",
    "#         img1, img2 = self.transform(image), self.transform(image)\n",
    "#         return img1, img2, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet = models.resnet50(weights=None)\n",
    "len(list(resnet.children())[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using downloadable resnet model without all WCL changes to architecture\n",
    "\n",
    "# replace the first layer with a smaller conv layer [kernel size = 3]\n",
    "# remove the final fc (linear) layer so the output is of size 2048\n",
    "\n",
    "class Net(nn.Module): \n",
    "    def __init__(self): \n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,  # 3 channels in\n",
    "                               stride=1, padding=1,   # 64 channels out\n",
    "                               bias = False)\n",
    "        layers = list(models.resnet50(weights=None).children())[1:-1]\n",
    "        self.middle = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.middle(x)\n",
    "        return x.view(x.shape[0], -1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 64, 32, 32])\n",
      "torch.Size([128, 2048, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 2048])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test Net\n",
    "\n",
    "test_model = Net()\n",
    "z = test_model.forward(x)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adapted from WCL but INCORRECT ## SEE NEXT \n",
    "class ProjectionHead(nn.Module): \n",
    "    def __init__(self, dim_in=2048, dim_out=128, dim_hidden=2048): \n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.linear1 = nn.Linear(dim_in, dim_hidden)\n",
    "        self.bn1 = nn.BatchNorm1d(dim_hidden)\n",
    "        self.linear2 = nn.Linear(dim_hidden, dim_hidden)\n",
    "        self.bn2 = nn.BatchNorm1d(dim_hidden)\n",
    "        self.linear3 = nn.Linear(dim_hidden, dim_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x).unsqueeze(-1).unsqueeze(-1) ## BN spatial reqs\n",
    "        x = self.bn1(x).squeeze(-1).squeeze(-1)\n",
    "        x = nn.ReLU(inplace=True)\n",
    "        x = self.linear2(x).unsqueeze(-1).unsqueeze(-1)\n",
    "        x = self.bn2(x).squeeze(-1).squeeze(-1)\n",
    "        x = nn.ReLU(inplace=True)\n",
    "        x = self.linear3(x)\n",
    "        return x               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why do we squeeze and unsqueeze?\n",
    "\n",
    "Batch normalization (BatchNorm) typically operates on mini-batches of data in convolutional neural networks (CNNs). In CNNs, the input to each layer is often represented as a 4D tensor with dimensions [batch_size, channels, height, width]. BatchNorm normalizes along the channel dimension, so it requires statistics to be computed across the batch and spatial dimensions while keeping the channel dimension intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case though, they are using BatchNorm1d \n",
    "#which accepts (N, C, L) or (N, C). Extra unsqueeze unnecessary?? \n",
    "\n",
    "test_lin = nn.Linear(2048, 2048)\n",
    "a = test_lin(z).unsqueeze(-1).unsqueeze(-1) # This does not work\n",
    "a.shape\n",
    "test_bn = nn.BatchNorm1d(2048)\n",
    "b = test_bn(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 2048])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = test_lin(z) ## this works\n",
    "b = test_bn(a)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CORRECT \n",
    "class ProjectionHead(nn.Module): \n",
    "    def __init__(self, dim_in=2048, dim_out=128, dim_hidden=2048): \n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.linear1 = nn.Linear(dim_in, dim_hidden)\n",
    "        self.bn1 = nn.BatchNorm1d(dim_hidden)\n",
    "        self.linear2 = nn.Linear(dim_hidden, dim_hidden)\n",
    "        self.bn2 = nn.BatchNorm1d(dim_hidden)\n",
    "        self.linear3 = nn.Linear(dim_hidden, dim_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.linear2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2048])\n",
      "torch.Size([128, 2048])\n",
      "torch.Size([128, 2048])\n",
      "torch.Size([128, 2048])\n",
      "torch.Size([128, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = ProjectionHead()\n",
    "a = test_model.forward(z)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question\n",
    "\n",
    "Purpose of @torch.no_grad()? Difference from model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WCL(nn.Module): \n",
    "    def __init__(self, dim_hidden=4096, dim_out=256):\n",
    "        super(WCL, self).__init__\n",
    "        self.net = Net()\n",
    "        self.head1 = ProjectionHead(dim_in=2048, \n",
    "                                    dim_out=dim_out, \n",
    "                                    dim_hidden=dim_hidden)\n",
    "        self.head1 = ProjectionHead(dim_in=2048, \n",
    "                                    dim_out=dim_out, \n",
    "                                    dim_hidden=dim_hidden)\n",
    "    \n",
    "    def build_connected_component(self, distances): \n",
    "        b = distances.shape[0]\n",
    "        distances = torch.eye(b, b) *2\n",
    "        # Returns a 2-D tensor with twos on the diagonal and zeros elsewhere.\n",
    "\n",
    "    def forward(self, x1, x2, t=0.1): \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
