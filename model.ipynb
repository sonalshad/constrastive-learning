{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tldr "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Components\n",
    "- one class for the pre-trained resnet model. \n",
    "- one class for the projection head. \n",
    "- put them together in simCLR class with a loss function (using pre-defined loss)\n",
    "- no optimizer rn (4/29). They use LARS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Projection Head\n",
    "Slightly different from the WCL implementation, only in terms of dimensions. The WCL model 'unsqueezes' twice, applies BatchNorm, and 'squeezes' twice.\n",
    "\n",
    "> Batch normalization (BatchNorm) typically operates on mini-batches of data in convolutional neural networks (CNNs). In CNNs, the input to each layer is often represented as a 4D tensor with dimensions [batch_size, channels, height, width]. BatchNorm normalizes along the channel dimension, so it requires statistics to be computed across the batch and spatial dimensions while keeping the channel dimension intact.`\n",
    "\n",
    "However, they also use BatchNorm1d, which does not require that. Moreover, adjusting dimensions does not work with BN1d. :( "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "Q: What does torch.no_grad() do that model.eval() does not?   \n",
    "Q: SimCLR uses pre-trained Resnet weights  \n",
    "Q: WCL model uses F.normalize after projection head  \n",
    "Q: SimCLR throws away projection head and uses hi for linear eval .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a Conv2D layer, input = (N * C * H * W) or (C * H * W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using downloadable resnet model without all WCL changes to architecture\n",
    "\n",
    "# replace the first layer with a smaller conv layer [kernel size = 3]\n",
    "# remove the final fc (linear) layer so the output is of size 2048\n",
    "\n",
    "class Net(nn.Module): \n",
    "    def __init__(self): \n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,    # In: 3 channels\n",
    "                               stride=1, padding=1,     # Out: 64 channels\n",
    "                               bias = False)\n",
    "        layers = list(models.resnet50().children())[1:-1] \n",
    "        self.middle = nn.Sequential(*layers) \n",
    "\n",
    "    def forward(self, x):  # [N * C * H * W]\n",
    "        x = self.conv1(x)  #  [N * 64 * H * W]\n",
    "        x = self.middle(x)  # [N * 2048] 2048 is resnet hidden dim\n",
    "        return x.view(x.shape[0], -1) # [N * 2048]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module): \n",
    "    def __init__(self, dim_in=2048, dim_out=128, dim_hidden=2048): \n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.linear1 = nn.Linear(dim_in, dim_hidden)\n",
    "        self.bn1 = nn.BatchNorm1d(dim_hidden)\n",
    "        self.linear2 = nn.Linear(dim_hidden, dim_hidden)\n",
    "        self.bn2 = nn.BatchNorm1d(dim_hidden)\n",
    "        self.linear3 = nn.Linear(dim_hidden, dim_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.linear2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contrastive Loss** defined on zi's per [original paper](https://arxiv.org/pdf/2002.05709v3): \n",
    "\n",
    "Contrastive loss function defined for a contrastive pre- diction task. Given a set {x ̃k} including a positive pair of examples x ̃i and x ̃j, the contrastive prediction task aims to identify x ̃j in {x ̃k}k̸=i for a given x ̃i.\n",
    "\n",
    "[Kevin Musgraves added NTXentLoss](https://github.com/KevinMusgrave/pytorch-metric-learning/issues/6)\n",
    "\n",
    "[Requirements](https://kevinmusgrave.github.io/pytorch-metric-learning/losses/#ntxentloss): Positive pairs (embeddings[i], embeddings[j]) are defined when labels[i]==labels[j]  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5128)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Example\n",
    "# ! pip install pytorch-metric-learning\n",
    "\n",
    "import torch\n",
    "from pytorch_metric_learning.losses import NTXentLoss\n",
    "\n",
    "batch_size = 16\n",
    "embedding_dim = 512\n",
    "\n",
    "# Just to make this example runnable\n",
    "anchor_embeddings = torch.randn(batch_size, embedding_dim)\n",
    "positive_embeddings = torch.randn(batch_size, embedding_dim)\n",
    "\n",
    "embeddings = torch.cat((anchor_embeddings, positive_embeddings))\n",
    "indices = torch.arange(0, anchor_embeddings.size(0), device=anchor_embeddings.device)\n",
    "labels = torch.cat((indices, indices))\n",
    "\n",
    "loss = NTXentLoss(temperature=0.10)\n",
    "loss(embeddings, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic Intuitive Model? \n",
    "\n",
    "class SimCLR(nn.Module): \n",
    "    def __init__(self): \n",
    "        super(SimCLR, self).__init__()\n",
    "        self.resnet = Net()\n",
    "        self.head = ProjectionHead()\n",
    "\n",
    "    def forward(self, x1, x2, t=0.1): \n",
    "        \n",
    "        h1 = self.resnet(x1)\n",
    "        h2 = self.resnet(x2)\n",
    "\n",
    "        ## As far as I can see, the WCL module applies F.normalize to these\n",
    "        ## maybe for computational complexity? \n",
    "        z1 = self.head(h1)\n",
    "        z2 = self.head(h2)     #dim = batch_size * embedding_size(128)\n",
    "\n",
    "        # N = batch_size * 2\n",
    "        # positive pairs identified by new labels (using index)\n",
    "        z = torch.cat((z1, z2))     #dim = N * embedding_size(128)\n",
    "        indices = torch.arange(0, z1.size(0)) \n",
    "        labels = torch.cat((indices, indices))      #dim = N\n",
    "        return z, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3, 10, 10]), torch.Size([5, 3, 10, 10]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## test dimensions\n",
    "batch_size = 5\n",
    "height, width = 10, 10\n",
    "channels = 3\n",
    "image1 = torch.randn(batch_size, channels, height, width)\n",
    "image2 = torch.randn(batch_size, channels, height, width)\n",
    "\n",
    "image1.shape, image2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 128]), torch.Size([10]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimCLR()\n",
    "z, labels = model.forward(image1, image2)\n",
    "z.shape, labels.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0625, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = NTXentLoss(temperature=0.10)\n",
    "loss(z, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
